{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e2bee4",
   "metadata": {
    "papermill": {
     "duration": 0.004882,
     "end_time": "2025-03-29T16:09:51.811401",
     "exception": false,
     "start_time": "2025-03-29T16:09:51.806519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "310717dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:09:56.288294Z",
     "iopub.status.busy": "2025-03-29T16:09:56.288019Z",
     "iopub.status.idle": "2025-03-29T16:10:09.673651Z",
     "shell.execute_reply": "2025-03-29T16:10:09.672920Z"
    },
    "papermill": {
     "duration": 13.392587,
     "end_time": "2025-03-29T16:10:09.675318",
     "exception": false,
     "start_time": "2025-03-29T16:09:56.282731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import os\n",
    "import tensorflow\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.signal import resample\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, BatchNormalization, LeakyReLU, Input, Add\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fba19095",
   "metadata": {
    "papermill": {
     "duration": 0.004522,
     "end_time": "2025-03-29T16:10:09.685069",
     "exception": false,
     "start_time": "2025-03-29T16:10:09.680547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Verify GPU\n",
    "A simple check to see if we use the GPU or not."
   ]
  },
  {
   "cell_type": "code",
   "id": "3082ffd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:10:09.695388Z",
     "iopub.status.busy": "2025-03-29T16:10:09.694887Z",
     "iopub.status.idle": "2025-03-29T16:10:10.330687Z",
     "shell.execute_reply": "2025-03-29T16:10:10.329841Z"
    },
    "papermill": {
     "duration": 0.64232,
     "end_time": "2025-03-29T16:10:10.331951",
     "exception": false,
     "start_time": "2025-03-29T16:10:09.689631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "if tensorflow.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU device found, using MPS for acceleration.\")\n",
    "else:\n",
    "    print(\"No GPU device found, falling back to CPU.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "24eefa59",
   "metadata": {
    "papermill": {
     "duration": 0.004519,
     "end_time": "2025-03-29T16:10:10.341588",
     "exception": false,
     "start_time": "2025-03-29T16:10:10.337069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Functions\n",
    "## map_labels\n",
    "This function can be used to simplify the labels. With a given label dictionary, we can map the labels to a less specific label."
   ]
  },
  {
   "cell_type": "code",
   "id": "5149a3c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:10:10.351837Z",
     "iopub.status.busy": "2025-03-29T16:10:10.351567Z",
     "iopub.status.idle": "2025-03-29T16:10:10.355603Z",
     "shell.execute_reply": "2025-03-29T16:10:10.354917Z"
    },
    "papermill": {
     "duration": 0.010774,
     "end_time": "2025-03-29T16:10:10.356907",
     "exception": false,
     "start_time": "2025-03-29T16:10:10.346133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def map_labels(data, label_dict, label_mapping):\n",
    "    \"\"\"\n",
    "    Maps annotations in a dataset to their corresponding labels using a given mapping.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): A DataFrame containing an 'annotation' column with values to be mapped.\n",
    "        label_dict (dict): A dictionary containing mappings of annotation labels.\n",
    "                                Keys are label mapping names, and values are Pandas Series or DataFrames\n",
    "                                where the index represents annotation keys and values represent labels.\n",
    "        label_mapping (str): The key in `anno_label_dict` corresponding to the desired label mapping.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The input DataFrame with an added or updated 'label' column containing the mapped labels.\n",
    "    \"\"\"\n",
    "    data['label'] = (label_dict[label_mapping].reindex(data['annotation']).to_numpy())\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c36db524",
   "metadata": {
    "papermill": {
     "duration": 0.004275,
     "end_time": "2025-03-29T16:10:10.365705",
     "exception": false,
     "start_time": "2025-03-29T16:10:10.361430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## normalize\n",
    "Normalize the data to make it better for generalization"
   ]
  },
  {
   "cell_type": "code",
   "id": "6d9ca429",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:10:10.375305Z",
     "iopub.status.busy": "2025-03-29T16:10:10.375100Z",
     "iopub.status.idle": "2025-03-29T16:10:10.378399Z",
     "shell.execute_reply": "2025-03-29T16:10:10.377816Z"
    },
    "papermill": {
     "duration": 0.009557,
     "end_time": "2025-03-29T16:10:10.379704",
     "exception": false,
     "start_time": "2025-03-29T16:10:10.370147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def normalize(data, feature_cols):\n",
    "    \"\"\"\n",
    "    Normalizes the specified feature columns of a DataFrame using Min-Max scaling.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): A DataFrame containing the features to be normalized.\n",
    "        feature_cols (list of str): List of column names in `data` to normalize.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the specified columns normalized to the range [0, 1].\n",
    "        The original `data` DataFrame is modified in place.\n",
    "        \n",
    "    Notes:\n",
    "        - Min-Max normalization scales each feature column to the range [0, 1] based on \n",
    "          the minimum and maximum values of the column.\n",
    "        - This transformation is often used to prepare data for machine learning models \n",
    "          that are sensitive to feature magnitudes.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    data[feature_cols] = scaler.fit_transform(data[feature_cols])\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "410789c5",
   "metadata": {
    "papermill": {
     "duration": 0.004387,
     "end_time": "2025-03-29T16:10:10.388812",
     "exception": false,
     "start_time": "2025-03-29T16:10:10.384425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## extract_windows\n",
    "A function to create windows. We can also change the frequency of the windows by downsampling."
   ]
  },
  {
   "cell_type": "code",
   "id": "4e3f4045",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:10:10.398551Z",
     "iopub.status.busy": "2025-03-29T16:10:10.398302Z",
     "iopub.status.idle": "2025-03-29T16:10:10.404213Z",
     "shell.execute_reply": "2025-03-29T16:10:10.403530Z"
    },
    "papermill": {
     "duration": 0.012199,
     "end_time": "2025-03-29T16:10:10.405508",
     "exception": false,
     "start_time": "2025-03-29T16:10:10.393309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def extract_windows(data, winsize=90, input_frequency=100, output_frequency=64):\n",
    "    \"\"\"\n",
    "    Extracts sliding windows from time series data and labels for classification tasks.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Time series DataFrame with 'x', 'y', 'z' (accelerometer data) and 'label'.\n",
    "                             Index should be a datetime-like index.\n",
    "        winsize (int): Window size in seconds.\n",
    "        input_frequency (int): Sampling frequency of input data in Hz.\n",
    "        output_frequency (int): Desired output frequency in Hz (must be a divisor of input_frequency).\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Shape (n_samples, output_samples, 3), accelerometer windows.\n",
    "        Y (np.ndarray): Shape (n_samples,), most frequent label per window.\n",
    "    \"\"\"\n",
    "    # Calculate window size in samples and target output samples\n",
    "    window_samples = winsize * input_frequency\n",
    "    output_samples = winsize * output_frequency  # Expected downsampled length\n",
    "\n",
    "    X, Y = [], []\n",
    "\n",
    "    # Sliding window extraction\n",
    "    for start in range(0, len(data) - window_samples + 1, window_samples):\n",
    "        window = data.iloc[start:start + window_samples]\n",
    "\n",
    "        # Skip if missing values exist\n",
    "        if window.isna().any().any() or len(window) != window_samples:\n",
    "            continue\n",
    "\n",
    "        # Extract and resample accelerometer data\n",
    "        x = window[['x', 'y', 'z']].to_numpy()\n",
    "        x = resample(x, output_samples)  # Resample to match output frequency\n",
    "\n",
    "        # Extract the most frequent label (mode)\n",
    "        y = window['label'].mode().iloc[0]\n",
    "        \n",
    "\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    X = np.stack(X) if X else np.empty((0, output_samples, 3))\n",
    "    Y = np.array(Y) if Y else np.empty((0,))\n",
    "\n",
    "    return X, Y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "491dbfc6",
   "metadata": {
    "papermill": {
     "duration": 0.004381,
     "end_time": "2025-03-29T16:10:10.414454",
     "exception": false,
     "start_time": "2025-03-29T16:10:10.410073",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## residual_block\n",
    "A function to create the residual blocks inside the model."
   ]
  },
  {
   "cell_type": "code",
   "id": "014e01c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:10:10.424132Z",
     "iopub.status.busy": "2025-03-29T16:10:10.423921Z",
     "iopub.status.idle": "2025-03-29T16:10:10.428347Z",
     "shell.execute_reply": "2025-03-29T16:10:10.427769Z"
    },
    "papermill": {
     "duration": 0.010701,
     "end_time": "2025-03-29T16:10:10.429630",
     "exception": false,
     "start_time": "2025-03-29T16:10:10.418929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def residual_block(inputs, filters, kernel_size, dropout_rate=0.5):\n",
    "    \"\"\"\n",
    "    Constructs a 1D residual block using Conv1D, BatchNormalization, and LeakyReLU layers.\n",
    "\n",
    "    Args:\n",
    "        inputs (tf.Tensor): Input tensor to the residual block (typically from a previous Conv1D layer).\n",
    "        filters (int): Number of filters for the Conv1D layers.\n",
    "        kernel_size (int): Size of the convolutional kernel.\n",
    "        dropout_rate (float, optional): Dropout rate applied after the first activation. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Output tensor after applying the residual block transformations.\n",
    "    \"\"\"\n",
    "    x = Conv1D(filters=filters, kernel_size=kernel_size, padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(negative_slope=0.1)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = Conv1D(filters=filters, kernel_size=kernel_size, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    shortcut = inputs\n",
    "    if inputs.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters=filters, kernel_size=1, padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    \n",
    "    x = Add()([x, shortcut])\n",
    "    x = LeakyReLU(negative_slope=0.1)(x)\n",
    "    return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dc1eece4",
   "metadata": {
    "papermill": {
     "duration": 0.004386,
     "end_time": "2025-03-29T16:10:10.438591",
     "exception": false,
     "start_time": "2025-03-29T16:10:10.434205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## create_model\n",
    "A function to create the model."
   ]
  },
  {
   "cell_type": "code",
   "id": "ab73bd69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:10:10.448324Z",
     "iopub.status.busy": "2025-03-29T16:10:10.448107Z",
     "iopub.status.idle": "2025-03-29T16:10:10.455303Z",
     "shell.execute_reply": "2025-03-29T16:10:10.454699Z"
    },
    "papermill": {
     "duration": 0.0135,
     "end_time": "2025-03-29T16:10:10.456563",
     "exception": false,
     "start_time": "2025-03-29T16:10:10.443063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def create_model(\n",
    "    num_classes, \n",
    "    input_shape, \n",
    "    dropout_rate,\n",
    "    filters, \n",
    "    kernel_size,\n",
    "    residual_blocks,\n",
    "    learning_rate\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds and compiles a deep neural network with multiple residual blocks.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of output classes for classification.\n",
    "        input_shape (tuple): Shape of the input data (excluding batch size), e.g., (timesteps, features).\n",
    "        dropout_rate (float): Dropout rate applied before the final classification layers for regularization.\n",
    "        filters (int): Number of filters to use in the initial convolutional layers; this scales up in deeper layers.\n",
    "        kernel_size (int): Size of the convolutional kernels used throughout the network.\n",
    "        residual_blocks (int): Number of residual blocks to use in the first and third residual block sets.\n",
    "        learning_rate (float): Learning rate for the Adam optimizer.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A compiled Keras model ready for training.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Initial Convolution\n",
    "    x = Conv1D(filters=filters, kernel_size=kernel_size, padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(negative_slope=0.1)(x)\n",
    "    x = MaxPooling1D(pool_size=3)(x)\n",
    "    \n",
    "    # First residual block set\n",
    "    for _ in range(residual_blocks):\n",
    "        x = residual_block(x, filters=filters, kernel_size=kernel_size)\n",
    "    x = MaxPooling1D(pool_size=3)(x)\n",
    "    \n",
    "    # Second residual block set\n",
    "    x = Conv1D(filters=filters * 2, kernel_size=kernel_size, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(negative_slope=0.1)(x)\n",
    "    for _ in range(3):\n",
    "        x = residual_block(x, filters=filters * 2, kernel_size=kernel_size)\n",
    "    x = MaxPooling1D(pool_size=5)(x)\n",
    "\n",
    "    # Third residual block set\n",
    "    for _ in range(residual_blocks):\n",
    "        x = residual_block(x, filters=filters * 2, kernel_size=kernel_size)\n",
    "    x = MaxPooling1D(pool_size=5)(x)\n",
    "\n",
    "    # Fourth residual block set (512 channels)\n",
    "    x = Conv1D(filters=filters * 4, kernel_size=kernel_size, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(negative_slope=0.1)(x)\n",
    "    for _ in range(3):\n",
    "        x = residual_block(x, filters=filters * 4, kernel_size=kernel_size)\n",
    "    x = MaxPooling1D(pool_size=5)(x)\n",
    "    \n",
    "    # BiLSTM Layer\n",
    "    x = Bidirectional(LSTM(filters * 8, return_sequences=False, dropout=dropout_rate))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    x = Dense(filters * 8, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Create and Compile Model\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate), \n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c576b967",
   "metadata": {
    "papermill": {
     "duration": 0.004506,
     "end_time": "2025-03-29T16:10:10.465513",
     "exception": false,
     "start_time": "2025-03-29T16:10:10.461007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Print input files"
   ]
  },
  {
   "cell_type": "code",
   "id": "f2c4fab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:10:10.475373Z",
     "iopub.status.busy": "2025-03-29T16:10:10.475165Z",
     "iopub.status.idle": "2025-03-29T16:10:10.541183Z",
     "shell.execute_reply": "2025-03-29T16:10:10.540452Z"
    },
    "papermill": {
     "duration": 0.072496,
     "end_time": "2025-03-29T16:10:10.542504",
     "exception": false,
     "start_time": "2025-03-29T16:10:10.470008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dictionary of labels\n",
    "label_dict = pd.read_csv('/kaggle/input/capture-24-human-activity-recognition/capture24/annotation-label-dictionary.csv', index_col='annotation', dtype='string')\n",
    "\n",
    "# Chosen label mapping\n",
    "label_mapping = \"label:Willetts2018\"\n",
    "print(label_dict[label_mapping])\n",
    "print()\n",
    "\n",
    "# Print files\n",
    "print('Content of data/')\n",
    "print(sorted(os.listdir('/kaggle/input/capture-24-human-activity-recognition/capture24')))\n",
    "print()\n",
    "\n",
    "# All of the labels\n",
    "all_labels = list({*label_dict[label_mapping]})\n",
    "print('All of the possible labels:')\n",
    "print(all_labels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bd8cb846",
   "metadata": {
    "papermill": {
     "duration": 0.004732,
     "end_time": "2025-03-29T16:10:10.552195",
     "exception": false,
     "start_time": "2025-03-29T16:10:10.547463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training the model\n",
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "id": "832eb557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:10:10.562252Z",
     "iopub.status.busy": "2025-03-29T16:10:10.562010Z",
     "iopub.status.idle": "2025-03-29T16:10:14.776207Z",
     "shell.execute_reply": "2025-03-29T16:10:14.775260Z"
    },
    "papermill": {
     "duration": 4.22261,
     "end_time": "2025-03-29T16:10:14.779405",
     "exception": false,
     "start_time": "2025-03-29T16:10:10.556795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Path of the dataset\n",
    "path = '/kaggle/input/capture-24-human-activity-recognition/capture24/'\n",
    "\n",
    "# The files in the dataset\n",
    "file_list = sorted([f for f in os.listdir(path) if f.endswith('.csv') and f.startswith('P')])\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "# Model parameters\n",
    "dropout_rate = 0.5\n",
    "filters = 128\n",
    "batch_size = 32\n",
    "kernel_size = 5\n",
    "residual_blocks = 3\n",
    "learning_rate = 3e-4\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Data and window parameters\n",
    "winsize = 90\n",
    "num_train_files = 100\n",
    "num_test_files = 51\n",
    "frequency = 64\n",
    "\n",
    "if num_train_files + num_test_files > 151:\n",
    "    raise Exception(\"Number of training and testing files exceeds the total number of files.\")\n",
    "\n",
    "# Choose train files\n",
    "train_files = file_list[:num_train_files]\n",
    "\n",
    "# Choose test files\n",
    "test_files = file_list[-num_test_files:]\n",
    "\n",
    "# The input shape for the model (window size * frequency, xyz)\n",
    "input_shape = (winsize * frequency, 3)\n",
    "\n",
    "# Create the model\n",
    "model = create_model(\n",
    "    num_classes=len(all_labels),\n",
    "    input_shape=input_shape,\n",
    "    dropout_rate=dropout_rate,\n",
    "    filters=filters,\n",
    "    kernel_size=kernel_size,\n",
    "    residual_blocks=residual_blocks,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n",
    "\n",
    "# Split the files into training and validation sets\n",
    "train_files, val_files = train_test_split(train_files, test_size=0.15, random_state=0)\n",
    "\n",
    "num_train_files = len(train_files)\n",
    "num_val_files = len(val_files)\n",
    "num_test_files = len(test_files)\n",
    "\n",
    "print(\"Number of files used for training:\", num_train_files)\n",
    "print(\"Number of files used for validation:\", num_val_files)\n",
    "print(\"Number of files used for testing:\", num_test_files)\n",
    "\n",
    "# Splitting the training files into chunks due to memory limitations\n",
    "num_chunks = 3\n",
    "num_chunks = min(num_chunks, num_train_files)\n",
    "print(\"Number of chunks used in training: \", num_chunks)\n",
    "\n",
    "# Size of each chunk\n",
    "chunk_size = len(train_files) // num_chunks\n",
    "# Remaining files that don't fit evenly into a chunk\n",
    "remaining_files = len(train_files) % num_chunks\n",
    "\n",
    "# Distribute remaining files across the chunks\n",
    "chunks = []\n",
    "for i in range(num_chunks):\n",
    "    # Give the extra files to the first few chunks\n",
    "    chunk = train_files[i * chunk_size: (i + 1) * chunk_size]\n",
    "    if i < remaining_files:\n",
    "        chunk.append(train_files[(num_chunks * chunk_size) + i])\n",
    "    chunks.append(chunk)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "24d786cd",
   "metadata": {
    "papermill": {
     "duration": 0.007543,
     "end_time": "2025-03-29T16:10:14.795188",
     "exception": false,
     "start_time": "2025-03-29T16:10:14.787645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Collecting the validation data"
   ]
  },
  {
   "cell_type": "code",
   "id": "77358fc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:10:14.809797Z",
     "iopub.status.busy": "2025-03-29T16:10:14.809520Z",
     "iopub.status.idle": "2025-03-29T16:17:29.708247Z",
     "shell.execute_reply": "2025-03-29T16:17:29.707302Z"
    },
    "papermill": {
     "duration": 434.91431,
     "end_time": "2025-03-29T16:17:29.716383",
     "exception": false,
     "start_time": "2025-03-29T16:10:14.802073",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Process validation data\n",
    "val_data_list = []\n",
    "for file in val_files:\n",
    "    file_path = os.path.join(path, file)\n",
    "    file_path = os.path.join(file_path, file)\n",
    "    data = pd.read_csv(file_path, index_col='time', parse_dates=['time'],\n",
    "                       dtype={'x': 'f4', 'y': 'f4', 'z': 'f4', 'annotation': 'string'})\n",
    "\n",
    "    data = map_labels(data, label_dict, label_mapping)\n",
    "    data = normalize(data, ['x', 'y', 'z'])\n",
    "    val_data_list.append(data)\n",
    "\n",
    "X_val_list, Y_val_list = [], []\n",
    "\n",
    "# Extract windows from validation data\n",
    "for data in val_data_list:\n",
    "    X_, Y_ = extract_windows(data)\n",
    "    X_val_list.append(X_)\n",
    "    Y_val_list.append(Y_)\n",
    "\n",
    "# Combine and encode validation data\n",
    "X_val = np.concatenate(X_val_list, axis=0)\n",
    "Y_val = np.concatenate(Y_val_list, axis=0)\n",
    "Y_val = label_encoder.transform(Y_val)\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "541b3355",
   "metadata": {
    "papermill": {
     "duration": 0.006973,
     "end_time": "2025-03-29T16:17:29.730621",
     "exception": false,
     "start_time": "2025-03-29T16:17:29.723648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training on the chunks"
   ]
  },
  {
   "cell_type": "code",
   "id": "a8f37bd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:17:29.746119Z",
     "iopub.status.busy": "2025-03-29T16:17:29.745798Z",
     "iopub.status.idle": "2025-03-29T19:27:59.749811Z",
     "shell.execute_reply": "2025-03-29T19:27:59.747963Z"
    },
    "papermill": {
     "duration": 11430.013487,
     "end_time": "2025-03-29T19:27:59.751335",
     "exception": false,
     "start_time": "2025-03-29T16:17:29.737848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "history_all = []\n",
    "\n",
    "# Iterate over the chunks\n",
    "for chunk_idx, chunk in enumerate(chunks):\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Processing chunk {chunk_idx + 1}/{len(chunks)}\")\n",
    "\n",
    "    data_list = []\n",
    "    \n",
    "    # Iterate over the files in the chunk\n",
    "    for file in chunk:\n",
    "        file_path = os.path.join(path, file)\n",
    "        file_path = os.path.join(file_path, file)\n",
    "        data = pd.read_csv(file_path, index_col='time', parse_dates=['time'],\n",
    "                           dtype={'x': 'f4', 'y': 'f4', 'z': 'f4', 'annotation': 'string'})\n",
    "\n",
    "        data = map_labels(data, label_dict, label_mapping)      \n",
    "        data = normalize(data, ['x','y','z'])\n",
    "        data_list.append(data)\n",
    "\n",
    "    X_chunk, Y_chunk = [], []\n",
    "\n",
    "    # Extract the windows from the data\n",
    "    for data in data_list:\n",
    "        X_, Y_ = extract_windows(data)\n",
    "        X_chunk.append(X_)\n",
    "        Y_chunk.append(Y_)\n",
    "\n",
    "    # A skip if the chunk is empty\n",
    "    if len(X_chunk) == 0 or len(Y_chunk) == 0:\n",
    "        print(f\"Error: No valid windows in chunk {chunk_idx}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    X_train = np.concatenate(X_chunk, axis=0)\n",
    "    Y_train = np.concatenate(Y_chunk, axis=0)\n",
    "\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Number of windows in chunk {chunk_idx + 1}: {X_train.shape[0]}\")\n",
    "\n",
    "    # Encode the labels\n",
    "    Y_train = label_encoder.transform(Y_train)\n",
    "\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    # Set up the early stopping\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "\n",
    "    # Fit the model with the labels\n",
    "    history = model.fit(\n",
    "        X_train, Y_train,\n",
    "        validation_data=(X_val, Y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    history_all.append(history.history)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2917b799",
   "metadata": {
    "papermill": {
     "duration": 0.014462,
     "end_time": "2025-03-29T19:27:59.781381",
     "exception": false,
     "start_time": "2025-03-29T19:27:59.766919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Accuary during Epochs"
   ]
  },
  {
   "cell_type": "code",
   "id": "0d140c41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T19:27:59.811971Z",
     "iopub.status.busy": "2025-03-29T19:27:59.811722Z",
     "iopub.status.idle": "2025-03-29T19:28:00.996703Z",
     "shell.execute_reply": "2025-03-29T19:28:00.995862Z"
    },
    "papermill": {
     "duration": 1.202177,
     "end_time": "2025-03-29T19:28:00.997988",
     "exception": false,
     "start_time": "2025-03-29T19:27:59.795811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Plot training and validation accuracy/loss per chunk\n",
    "for i, hist in enumerate(history_all):\n",
    "    epochs_range = range(1, len(hist['loss']) + 1)\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.figure()\n",
    "    plt.plot(epochs_range, hist['accuracy'], label='Train Accuracy')\n",
    "    if 'val_accuracy' in hist:\n",
    "        plt.plot(epochs_range, hist['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title(f'Chunk {i+1} - Accuracy per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Loss plot\n",
    "    plt.figure()\n",
    "    plt.plot(epochs_range, hist['loss'], label='Train Loss')\n",
    "    if 'val_loss' in hist:\n",
    "        plt.plot(epochs_range, hist['val_loss'], label='Val Loss')\n",
    "    plt.title(f'Chunk {i+1} - Loss per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b1411df0",
   "metadata": {
    "papermill": {
     "duration": 0.019297,
     "end_time": "2025-03-29T19:28:01.039015",
     "exception": false,
     "start_time": "2025-03-29T19:28:01.019718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "bb9e51c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T19:28:01.078749Z",
     "iopub.status.busy": "2025-03-29T19:28:01.078506Z",
     "iopub.status.idle": "2025-03-29T19:53:31.521715Z",
     "shell.execute_reply": "2025-03-29T19:53:31.520857Z"
    },
    "papermill": {
     "duration": 1530.464931,
     "end_time": "2025-03-29T19:53:31.523159",
     "exception": false,
     "start_time": "2025-03-29T19:28:01.058228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Split the test files into two halves\n",
    "midpoint = len(test_files) // 2\n",
    "file_batches = [test_files[:midpoint], test_files[midpoint:]]\n",
    "\n",
    "X_test, Y_test = [], []\n",
    "\n",
    "# Process each half separately\n",
    "for batch_i, file_batch in enumerate(file_batches, 1):\n",
    "    print(f\"\\n--- Processing batch {batch_i}/{len(file_batches)} ---\")\n",
    "    data_list = []\n",
    "    \n",
    "    for file_i, file in enumerate(file_batch, 1):\n",
    "        print(f\"  File {file_i}/{len(file_batch)}\")\n",
    "        file_path = os.path.join(path, file)\n",
    "        file_path = os.path.join(file_path, file)\n",
    "        data = pd.read_csv(\n",
    "            file_path, index_col='time', parse_dates=['time'],\n",
    "            dtype={'x': 'f4', 'y': 'f4', 'z': 'f4', 'annotation': 'string'}\n",
    "        )\n",
    "\n",
    "        data = map_labels(data, label_dict, label_mapping)\n",
    "        data = normalize(data, ['x', 'y', 'z'])\n",
    "        data_list.append(data)\n",
    "\n",
    "    # Extract windows and free memory\n",
    "    for data in data_list:\n",
    "        X_, Y_ = extract_windows(data)\n",
    "        X_test.append(X_)\n",
    "        Y_test.append(Y_)\n",
    "    \n",
    "    del data_list\n",
    "    gc.collect()\n",
    "\n",
    "# Combine both halves\n",
    "X_test = np.concatenate(X_test, axis=0)\n",
    "Y_test = np.concatenate(Y_test, axis=0)\n",
    "\n",
    "# Make predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "Y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# Decode predictions\n",
    "Y_pred = label_encoder.inverse_transform(Y_pred)\n",
    "\n",
    "# Calculate metrics\n",
    "acc = accuracy_score(Y_test, Y_pred)\n",
    "f1 = f1_score(Y_test, Y_pred, average='weighted', labels=all_labels)\n",
    "recall = recall_score(Y_test, Y_pred, average='weighted', labels=all_labels)\n",
    "precision = precision_score(Y_test, Y_pred, average='weighted', labels=all_labels)\n",
    "\n",
    "# Confusion matrix for present labels\n",
    "cm = confusion_matrix(Y_test, Y_pred, labels=all_labels, normalize='true')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=all_labels)\n",
    "disp.plot(cmap='Blues', values_format=\".2%\")\n",
    "disp.ax_.set_title(\"Confusion Matrix (Percentage)\")\n",
    "\n",
    "# Print metrics\n",
    "print(f\"  Accuracy: {acc:.4f}\")\n",
    "print(f\"  F1 Score: {f1:.4f}\")\n",
    "print(f\"  Recall: {recall:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e11a8ecc",
   "metadata": {
    "papermill": {
     "duration": 0.043923,
     "end_time": "2025-03-29T19:53:31.612974",
     "exception": false,
     "start_time": "2025-03-29T19:53:31.569051",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "3346ab30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T19:53:31.701747Z",
     "iopub.status.busy": "2025-03-29T19:53:31.701419Z",
     "iopub.status.idle": "2025-03-29T19:53:32.973622Z",
     "shell.execute_reply": "2025-03-29T19:53:32.972874Z"
    },
    "papermill": {
     "duration": 1.318264,
     "end_time": "2025-03-29T19:53:32.975252",
     "exception": false,
     "start_time": "2025-03-29T19:53:31.656988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Save the model\n",
    "model.save(\"/kaggle/working/RNN.keras\")\n",
    "\n",
    "# Save the label encoder\n",
    "np.save('classes.npy', label_encoder.classes_)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5173716,
     "sourceId": 8639177,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13427.424033,
   "end_time": "2025-03-29T19:53:36.630160",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-29T16:09:49.206127",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
