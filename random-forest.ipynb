{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbed0a5",
   "metadata": {
    "papermill": {
     "duration": 0.006528,
     "end_time": "2025-03-30T20:35:46.850575",
     "exception": false,
     "start_time": "2025-03-30T20:35:46.844047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "1ac0c90e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T20:36:03.763766Z",
     "iopub.status.busy": "2025-03-30T20:36:03.763364Z",
     "iopub.status.idle": "2025-03-30T20:36:06.939107Z",
     "shell.execute_reply": "2025-03-30T20:36:06.937395Z"
    },
    "papermill": {
     "duration": 3.18587,
     "end_time": "2025-03-30T20:36:06.941315",
     "exception": false,
     "start_time": "2025-03-30T20:36:03.755445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "\n",
    "from scipy.signal import resample\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as signal"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bcee632b",
   "metadata": {
    "papermill": {
     "duration": 0.007965,
     "end_time": "2025-03-30T20:36:06.957452",
     "exception": false,
     "start_time": "2025-03-30T20:36:06.949487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Functions\n",
    "## map_labels"
   ]
  },
  {
   "cell_type": "code",
   "id": "8563db76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T20:36:06.972821Z",
     "iopub.status.busy": "2025-03-30T20:36:06.972231Z",
     "iopub.status.idle": "2025-03-30T20:36:06.977966Z",
     "shell.execute_reply": "2025-03-30T20:36:06.976611Z"
    },
    "papermill": {
     "duration": 0.015499,
     "end_time": "2025-03-30T20:36:06.979949",
     "exception": false,
     "start_time": "2025-03-30T20:36:06.964450",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def map_labels(data, label_dict, label_mapping):\n",
    "    \"\"\"\n",
    "    Maps annotations in a dataset to their corresponding labels using a given mapping.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): A DataFrame containing an 'annotation' column with values to be mapped.\n",
    "        label_dict (dict): A dictionary containing mappings of annotation labels.\n",
    "                                Keys are label mapping names, and values are Pandas Series or DataFrames\n",
    "                                where the index represents annotation keys and values represent labels.\n",
    "        label_mapping (str): The key in `anno_label_dict` corresponding to the desired label mapping.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The input DataFrame with an added or updated 'label' column containing the mapped labels.\n",
    "    \"\"\"\n",
    "    data['label'] = (label_dict[label_mapping].reindex(data['annotation']).to_numpy())\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f59f3486",
   "metadata": {
    "papermill": {
     "duration": 0.006929,
     "end_time": "2025-03-30T20:36:06.994610",
     "exception": false,
     "start_time": "2025-03-30T20:36:06.987681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## normalize"
   ]
  },
  {
   "cell_type": "code",
   "id": "65cd5c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T20:36:07.012638Z",
     "iopub.status.busy": "2025-03-30T20:36:07.012201Z",
     "iopub.status.idle": "2025-03-30T20:36:07.017479Z",
     "shell.execute_reply": "2025-03-30T20:36:07.016155Z"
    },
    "papermill": {
     "duration": 0.016805,
     "end_time": "2025-03-30T20:36:07.020041",
     "exception": false,
     "start_time": "2025-03-30T20:36:07.003236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def normalize(data, feature_cols):\n",
    "    \"\"\"\n",
    "    Normalizes the specified feature columns of a DataFrame using Min-Max scaling.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): A DataFrame containing the features to be normalized.\n",
    "        feature_cols (list of str): List of column names in `data` to normalize.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the specified columns normalized to the range [0, 1].\n",
    "        The original `data` DataFrame is modified in place.\n",
    "        \n",
    "    Notes:\n",
    "        - Min-Max normalization scales each feature column to the range [0, 1] based on \n",
    "          the minimum and maximum values of the column.\n",
    "        - This transformation is often used to prepare data for machine learning models \n",
    "          that are sensitive to feature magnitudes.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    data[feature_cols] = scaler.fit_transform(data[feature_cols])\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5c6fe3c1",
   "metadata": {
    "papermill": {
     "duration": 0.00672,
     "end_time": "2025-03-30T20:36:07.035212",
     "exception": false,
     "start_time": "2025-03-30T20:36:07.028492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## extract_windows"
   ]
  },
  {
   "cell_type": "code",
   "id": "9181c3ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T20:36:07.051334Z",
     "iopub.status.busy": "2025-03-30T20:36:07.050963Z",
     "iopub.status.idle": "2025-03-30T20:36:07.058498Z",
     "shell.execute_reply": "2025-03-30T20:36:07.057352Z"
    },
    "papermill": {
     "duration": 0.017764,
     "end_time": "2025-03-30T20:36:07.060656",
     "exception": false,
     "start_time": "2025-03-30T20:36:07.042892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def extract_windows(data, winsize=90, input_frequency=100, output_frequency=64):\n",
    "    \"\"\"\n",
    "    Extracts sliding windows from time series data and labels for classification tasks.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Time series DataFrame with 'x', 'y', 'z' (accelerometer data) and 'label'.\n",
    "                             Index should be a datetime-like index.\n",
    "        winsize (int): Window size in seconds.\n",
    "        input_frequency (int): Sampling frequency of input data in Hz.\n",
    "        output_frequency (int): Desired output frequency in Hz (must be a divisor of input_frequency).\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Shape (n_samples, output_samples, 3), accelerometer windows.\n",
    "        Y (np.ndarray): Shape (n_samples,), most frequent label per window.\n",
    "    \"\"\"\n",
    "    # Calculate window size in samples and target output samples\n",
    "    window_samples = winsize * input_frequency\n",
    "    output_samples = winsize * output_frequency  # Expected downsampled length\n",
    "\n",
    "    X, Y = [], []\n",
    "\n",
    "    # Sliding window extraction\n",
    "    for start in range(0, len(data) - window_samples + 1, window_samples):\n",
    "        window = data.iloc[start:start + window_samples]\n",
    "\n",
    "        # Skip if missing values exist\n",
    "        if window.isna().any().any() or len(window) != window_samples:\n",
    "            continue\n",
    "\n",
    "        # Extract and resample accelerometer data\n",
    "        x = window[['x', 'y', 'z']].to_numpy()\n",
    "        x = resample(x, output_samples)  # Resample to match output frequency\n",
    "\n",
    "        # Extract the most frequent label (mode)\n",
    "        y = window['label'].mode().iloc[0]\n",
    "\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    X = np.stack(X) if X else np.empty((0, output_samples, 3))\n",
    "    Y = np.array(Y) if Y else np.empty((0,))\n",
    "\n",
    "    return X, Y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5ebb3d54",
   "metadata": {
    "papermill": {
     "duration": 0.006627,
     "end_time": "2025-03-30T20:36:07.074414",
     "exception": false,
     "start_time": "2025-03-30T20:36:07.067787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## extract_features"
   ]
  },
  {
   "cell_type": "code",
   "id": "92b6588e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T20:36:07.091267Z",
     "iopub.status.busy": "2025-03-30T20:36:07.090582Z",
     "iopub.status.idle": "2025-03-30T20:36:07.112354Z",
     "shell.execute_reply": "2025-03-30T20:36:07.111350Z"
    },
    "papermill": {
     "duration": 0.032703,
     "end_time": "2025-03-30T20:36:07.114218",
     "exception": false,
     "start_time": "2025-03-30T20:36:07.081515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def extract_features(xyz, sample_rate=64):\n",
    "    \"\"\"Extract commonly used HAR time-series features. xyz is a window of shape (N,3)\"\"\"\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    x, y, z = xyz.T\n",
    "\n",
    "    feats['xmin'], feats['xq25'], feats['xmed'], feats['xq75'], feats['xmax'] = np.quantile(x, (0, .25, .5, .75, 1))\n",
    "    feats['ymin'], feats['yq25'], feats['ymed'], feats['yq75'], feats['ymax'] = np.quantile(y, (0, .25, .5, .75, 1))\n",
    "    feats['zmin'], feats['zq25'], feats['zmed'], feats['zq75'], feats['zmax'] = np.quantile(z, (0, .25, .5, .75, 1))\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):  # ignore div by 0 warnings\n",
    "        # xy, xy, zx correlation\n",
    "        feats['xycorr'] = np.nan_to_num(np.corrcoef(x, y)[0, 1])\n",
    "        feats['yzcorr'] = np.nan_to_num(np.corrcoef(y, z)[0, 1])\n",
    "        feats['zxcorr'] = np.nan_to_num(np.corrcoef(z, x)[0, 1])\n",
    "\n",
    "    v = np.linalg.norm(xyz, axis=1)\n",
    "\n",
    "    feats['min'], feats['q25'], feats['med'], feats['q75'], feats['max'] = np.quantile(v, (0, .25, .5, .75, 1))\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):  # ignore div by 0 warnings\n",
    "        # 1s autocorrelation\n",
    "        feats['corr1s'] = np.nan_to_num(np.corrcoef(v[:-sample_rate], v[sample_rate:]))[0, 1]\n",
    "\n",
    "    # Angular features\n",
    "    feats.update(angular_features(xyz, sample_rate))\n",
    "\n",
    "    # Spectral features\n",
    "    feats.update(spectral_features(v, sample_rate))\n",
    "\n",
    "    # Peak features\n",
    "    feats.update(peak_features(v, sample_rate))\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def spectral_features(v, sample_rate):\n",
    "    \"\"\" Spectral entropy, 1st & 2nd dominant frequencies \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    # Spectrum using Welch's method with 3s segment length\n",
    "    # First run without detrending to get the true spectrum\n",
    "    freqs, powers = signal.welch(v, fs=sample_rate,\n",
    "                                 nperseg=3 * sample_rate,\n",
    "                                 noverlap=2 * sample_rate,\n",
    "                                 detrend=False,\n",
    "                                 average='median')\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):  # ignore div by 0 warnings\n",
    "        feats['pentropy'] = np.nan_to_num(stats.entropy(powers + 1e-16))\n",
    "\n",
    "    # Spectrum using Welch's method with 3s segment length\n",
    "    # Now do detrend to focus on the relevant freqs\n",
    "    freqs, powers = signal.welch(v, fs=sample_rate,\n",
    "                                 nperseg=3 * sample_rate,\n",
    "                                 noverlap=2 * sample_rate,\n",
    "                                 detrend='constant',\n",
    "                                 average='median')\n",
    "\n",
    "    peaks, _ = signal.find_peaks(powers)\n",
    "    peak_powers = powers[peaks]\n",
    "    peak_freqs = freqs[peaks]\n",
    "    peak_ranks = np.argsort(peak_powers)[::-1]\n",
    "    if len(peaks) >= 2:\n",
    "        feats['f1'] = peak_freqs[peak_ranks[0]]\n",
    "        feats['f2'] = peak_freqs[peak_ranks[1]]\n",
    "        feats['p1'] = peak_powers[peak_ranks[0]]\n",
    "        feats['p2'] = peak_powers[peak_ranks[1]]\n",
    "    elif len(peaks) == 1:\n",
    "        feats['f1'] = feats['f2'] = peak_freqs[peak_ranks[0]]\n",
    "        feats['p1'] = feats['p2'] = peak_powers[peak_ranks[0]]\n",
    "    else:\n",
    "        feats['f1'] = feats['f2'] = 0\n",
    "        feats['p1'] = feats['p2'] = 0\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def peak_features(v, sample_rate):\n",
    "    \"\"\" Features of the signal peaks. A proxy to step counts. \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "    u = butterfilt(v, (.6, 5), fs=sample_rate)\n",
    "    peaks, peak_props = signal.find_peaks(u, distance=0.2 * sample_rate, prominence=0.25)\n",
    "    feats['numPeaks'] = len(peaks)\n",
    "    if len(peak_props['prominences']) > 0:\n",
    "        feats['peakPromin'] = np.median(peak_props['prominences'])\n",
    "    else:\n",
    "        feats['peakPromin'] = 0\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def angular_features(xyz, sample_rate):\n",
    "    \"\"\" Roll, pitch, yaw.\n",
    "    Hip and Wrist Accelerometer Algorithms for Free-Living Behavior\n",
    "    Classification, Ellis et al.\n",
    "    \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    # Raw angles\n",
    "    x, y, z = xyz.T\n",
    "\n",
    "    roll = np.arctan2(y, z)\n",
    "    pitch = np.arctan2(x, z)\n",
    "    yaw = np.arctan2(y, x)\n",
    "\n",
    "    feats['avgroll'] = np.mean(roll)\n",
    "    feats['avgpitch'] = np.mean(pitch)\n",
    "    feats['avgyaw'] = np.mean(yaw)\n",
    "    feats['sdroll'] = np.std(roll)\n",
    "    feats['sdpitch'] = np.std(pitch)\n",
    "    feats['sdyaw'] = np.std(yaw)\n",
    "\n",
    "    # Gravity angles\n",
    "    xyz = butterfilt(xyz, 0.5, fs=sample_rate)\n",
    "\n",
    "    x, y, z = xyz.T\n",
    "\n",
    "    roll = np.arctan2(y, z)\n",
    "    pitch = np.arctan2(x, z)\n",
    "    yaw = np.arctan2(y, x)\n",
    "\n",
    "    feats['rollg'] = np.mean(roll)\n",
    "    feats['pitchg'] = np.mean(pitch)\n",
    "    feats['yawg'] = np.mean(yaw)\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def butterfilt(x, cutoffs, fs, order=10, axis=0):\n",
    "    nyq = 0.5 * fs\n",
    "    if isinstance(cutoffs, tuple):\n",
    "        hicut, lowcut = cutoffs\n",
    "        if hicut > 0:\n",
    "            btype = 'bandpass'\n",
    "            Wn = (hicut / nyq, lowcut / nyq)\n",
    "        else:\n",
    "            btype = 'low'\n",
    "            Wn = lowcut / nyq\n",
    "    else:\n",
    "        btype = 'low'\n",
    "        Wn = cutoffs / nyq\n",
    "    sos = signal.butter(order, Wn, btype=btype, analog=False, output='sos')\n",
    "    y = signal.sosfiltfilt(sos, x, axis=axis)\n",
    "    return y\n",
    "\n",
    "\n",
    "def get_feature_names():\n",
    "    \"\"\" Hacky way to get the list of feature names \"\"\"\n",
    "\n",
    "    feats = extract_features(np.zeros((1000, 3)), 100)\n",
    "    return list(feats.keys())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e2cc9820",
   "metadata": {
    "papermill": {
     "duration": 0.006483,
     "end_time": "2025-03-30T20:36:07.128124",
     "exception": false,
     "start_time": "2025-03-30T20:36:07.121641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Print input files"
   ]
  },
  {
   "cell_type": "code",
   "id": "cd3c214d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T20:36:07.143402Z",
     "iopub.status.busy": "2025-03-30T20:36:07.143053Z",
     "iopub.status.idle": "2025-03-30T20:36:07.220117Z",
     "shell.execute_reply": "2025-03-30T20:36:07.218902Z"
    },
    "papermill": {
     "duration": 0.086698,
     "end_time": "2025-03-30T20:36:07.221981",
     "exception": false,
     "start_time": "2025-03-30T20:36:07.135283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dictionary of labels\n",
    "label_dict = pd.read_csv('/kaggle/input/capture-24-human-activity-recognition/capture24/annotation-label-dictionary.csv', index_col='annotation', dtype='string')\n",
    "print(\"Annotation-Label Dictionary\")\n",
    "print(label_dict)\n",
    "\n",
    "# Chosen label mapping\n",
    "label_mapping = \"label:Willetts2018\"\n",
    "print(label_dict[label_mapping])\n",
    "print()\n",
    "\n",
    "# Print files\n",
    "print('Content of data/')\n",
    "print(sorted(os.listdir('/kaggle/input/capture-24-human-activity-recognition/capture24')))\n",
    "print()\n",
    "\n",
    "# All of the labels\n",
    "all_labels = list({*label_dict[label_mapping]})\n",
    "print('All of the possible labels:')\n",
    "print(all_labels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "60409f18",
   "metadata": {
    "papermill": {
     "duration": 0.006747,
     "end_time": "2025-03-30T20:36:07.235955",
     "exception": false,
     "start_time": "2025-03-30T20:36:07.229208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training the model\n",
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "id": "4a7b370e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T20:36:07.251820Z",
     "iopub.status.busy": "2025-03-30T20:36:07.251358Z",
     "iopub.status.idle": "2025-03-30T20:36:07.259722Z",
     "shell.execute_reply": "2025-03-30T20:36:07.258597Z"
    },
    "papermill": {
     "duration": 0.018323,
     "end_time": "2025-03-30T20:36:07.261607",
     "exception": false,
     "start_time": "2025-03-30T20:36:07.243284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Path of the dataset\n",
    "path = '/kaggle/input/capture-24-human-activity-recognition/capture24/'\n",
    "\n",
    "# The files in the dataset\n",
    "file_list = sorted([f for f in os.listdir(path) if f.endswith('.csv') and f.startswith('P')])\n",
    "\n",
    "# Data and window parameters\n",
    "winsize = 90\n",
    "num_train_files = 100\n",
    "num_test_files = 51\n",
    "frequency = 64\n",
    "\n",
    "if num_train_files + num_test_files > 151:\n",
    "    raise Exception(\"Number of training and testing files exceeds the total number of files.\")\n",
    "\n",
    "# Choose test files\n",
    "test_files = file_list[-num_test_files:]\n",
    "\n",
    "# Choose train files\n",
    "train_files = file_list[:num_train_files]\n",
    "\n",
    "num_train_files = len(train_files)\n",
    "num_test_files = len(test_files)\n",
    "\n",
    "print(\"Number of files used for training:\", num_train_files)\n",
    "print(\"Number of files used for testing:\", num_test_files)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8bdb7deb",
   "metadata": {
    "papermill": {
     "duration": 0.006656,
     "end_time": "2025-03-30T20:36:07.275493",
     "exception": false,
     "start_time": "2025-03-30T20:36:07.268837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "id": "291f9fa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T20:36:07.291715Z",
     "iopub.status.busy": "2025-03-30T20:36:07.291286Z",
     "iopub.status.idle": "2025-03-30T21:55:33.734594Z",
     "shell.execute_reply": "2025-03-30T21:55:33.731973Z"
    },
    "papermill": {
     "duration": 4766.467365,
     "end_time": "2025-03-30T21:55:33.750162",
     "exception": false,
     "start_time": "2025-03-30T20:36:07.282797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load and process data in chunks to manage memory\n",
    "data_list = []\n",
    "counter = 0\n",
    "\n",
    "X, Y = [], []\n",
    "\n",
    "# Load and concatenate data from all files\n",
    "for file in train_files:\n",
    "    counter += 1\n",
    "    print(\"Processing File:\", counter)\n",
    "    \n",
    "    file_path = os.path.join(path, file)\n",
    "    file_path = os.path.join(file_path, file)\n",
    "    data = pd.read_csv(file_path, index_col='time', parse_dates=['time'],\n",
    "                           dtype={'x': 'f4', 'y': 'f4', 'z': 'f4', 'annotation': 'string'})\n",
    "    \n",
    "    data = map_labels(data, label_dict, label_mapping)      \n",
    "    data = normalize(data, ['x','y','z'])\n",
    "    \n",
    "    X_, Y_ = extract_windows(data)\n",
    "    del data\n",
    "    \n",
    "    X_feats = pd.DataFrame([extract_features(x) for x in X_])\n",
    "    del X_\n",
    "    \n",
    "    X.append(X_feats)\n",
    "    Y.append(Y_)\n",
    "    \n",
    "    del X_feats, Y_\n",
    "\n",
    "# Concatenate all data\n",
    "X_feats_train = pd.concat(X, ignore_index=True)\n",
    "Y_train = np.concatenate(Y, axis=0)\n",
    "\n",
    "del X, Y\n",
    "\n",
    "print('Data preprocessing done.\\n')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2e8c93dc",
   "metadata": {
    "papermill": {
     "duration": 0.013708,
     "end_time": "2025-03-30T21:55:33.779366",
     "exception": false,
     "start_time": "2025-03-30T21:55:33.765658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "3bf8c8fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T21:55:33.809599Z",
     "iopub.status.busy": "2025-03-30T21:55:33.809183Z",
     "iopub.status.idle": "2025-03-30T22:18:52.939611Z",
     "shell.execute_reply": "2025-03-30T22:18:52.937507Z"
    },
    "papermill": {
     "duration": 1399.169447,
     "end_time": "2025-03-30T22:18:52.962745",
     "exception": false,
     "start_time": "2025-03-30T21:55:33.793298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "\n",
    "model = BalancedRandomForestClassifier(\n",
    "    random_state=0, \n",
    "    n_estimators=3000, \n",
    "    criterion='gini'\n",
    ")\n",
    "\n",
    "# Use Stratified K-Fold Cross-Validation\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    estimator=model,\n",
    "    X=X_feats_train,\n",
    "    y=Y_train,\n",
    "    scoring='accuracy',\n",
    "    cv=cv,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(f'Cross-validation scores: {cv_scores}')\n",
    "print(f'Mean cross-validation score: {cv_scores.mean()}')\n",
    "print(f'Standard deviation of cross-validation score: {cv_scores.std()}')\n",
    "\n",
    "model.classes_ = np.array(all_labels)\n",
    "\n",
    "model.fit(X_feats_train, Y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8f0c227",
   "metadata": {
    "papermill": {
     "duration": 0.015448,
     "end_time": "2025-03-30T22:18:52.994432",
     "exception": false,
     "start_time": "2025-03-30T22:18:52.978984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ee8af",
   "metadata": {
    "papermill": {
     "duration": 0.016901,
     "end_time": "2025-03-30T22:18:53.027675",
     "exception": false,
     "start_time": "2025-03-30T22:18:53.010774",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "id": "62c16e09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T22:18:53.061241Z",
     "iopub.status.busy": "2025-03-30T22:18:53.060797Z",
     "iopub.status.idle": "2025-03-30T23:07:43.691560Z",
     "shell.execute_reply": "2025-03-30T23:07:43.689320Z"
    },
    "papermill": {
     "duration": 2930.676772,
     "end_time": "2025-03-30T23:07:43.720273",
     "exception": false,
     "start_time": "2025-03-30T22:18:53.043501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load and concatenate data from all files\n",
    "data_list = []\n",
    "counter = 0\n",
    "\n",
    "# Load and process data in chunks to manage memory\n",
    "for file in test_files:\n",
    "    counter += 1\n",
    "    print(\"Processing File:\", counter)\n",
    "    \n",
    "    file_path = os.path.join(path, file)\n",
    "    file_path = os.path.join(file_path, file)\n",
    "    data = pd.read_csv(file_path, index_col='time', parse_dates=['time'],\n",
    "                           dtype={'x': 'f4', 'y': 'f4', 'z': 'f4', 'annotation': 'string'})\n",
    "\n",
    "    data = map_labels(data, label_dict, label_mapping)      \n",
    "    data = normalize(data, ['x','y','z'])\n",
    "    data_list.append(data)\n",
    "\n",
    "    # Map the label\n",
    "    map_labels(data, label_dict, label_mapping)\n",
    "    data_list.append(data)\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Extract windows and features\n",
    "for data in data_list:\n",
    "    X_, Y_ = extract_windows(data)\n",
    "    X.append(X_)\n",
    "    Y.append(Y_)\n",
    "\n",
    "X_feats = []\n",
    "\n",
    "# Extract features\n",
    "for X_ in X:\n",
    "    X_feats.append(pd.DataFrame([\n",
    "        extract_features(x) for x in X_]))\n",
    "\n",
    "X_feats_test = pd.concat(X_feats, ignore_index=True)\n",
    "Y_test = np.concatenate(Y, axis=0)\n",
    "\n",
    "print('Data preprocessing done.\\n')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "deded92a",
   "metadata": {
    "papermill": {
     "duration": 0.020122,
     "end_time": "2025-03-30T23:07:43.760403",
     "exception": false,
     "start_time": "2025-03-30T23:07:43.740281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "16121682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T23:07:43.811965Z",
     "iopub.status.busy": "2025-03-30T23:07:43.811373Z",
     "iopub.status.idle": "2025-03-30T23:08:31.904188Z",
     "shell.execute_reply": "2025-03-30T23:08:31.902889Z"
    },
    "papermill": {
     "duration": 48.120008,
     "end_time": "2025-03-30T23:08:31.906283",
     "exception": false,
     "start_time": "2025-03-30T23:07:43.786275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Y_pred = model.predict(X_feats_test)\n",
    "\n",
    "# Calculate the important metrics\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "f1_score = f1_score(Y_test, Y_pred, average='weighted')\n",
    "precision = precision_score(Y_test, Y_pred, average='weighted')\n",
    "recall = recall_score(Y_test, Y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 score: {model.score(X_feats_test, Y_test)}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'Confusion matrix: {model.classes_}')\n",
    "\n",
    "# Subset labels present in the test set\n",
    "present_labels = np.unique(Y_test)\n",
    "\n",
    "# Confusion matrix for present labels\n",
    "cm = confusion_matrix(Y_test, Y_pred, labels=present_labels, normalize='true')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=present_labels)\n",
    "disp.plot(cmap='Blues', values_format=\".2%\")\n",
    "disp.ax_.set_title(\"Confusion Matrix (Percentage)\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4c24819c",
   "metadata": {
    "papermill": {
     "duration": 0.019014,
     "end_time": "2025-03-30T23:08:32.124958",
     "exception": false,
     "start_time": "2025-03-30T23:08:32.105944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "11c41112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T23:08:32.166002Z",
     "iopub.status.busy": "2025-03-30T23:08:32.165587Z",
     "iopub.status.idle": "2025-03-30T23:08:38.633363Z",
     "shell.execute_reply": "2025-03-30T23:08:38.631584Z"
    },
    "papermill": {
     "duration": 6.492501,
     "end_time": "2025-03-30T23:08:38.635994",
     "exception": false,
     "start_time": "2025-03-30T23:08:32.143493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Save the model\n",
    "with open('rf_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5173716,
     "sourceId": 8639177,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9178.189447,
   "end_time": "2025-03-30T23:08:41.894814",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-30T20:35:43.705367",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
