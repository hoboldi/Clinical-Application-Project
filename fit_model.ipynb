{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd221c8322933cd6",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628b6e9c35832aa5",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import unisens\n",
    "from scipy.signal import resample\n",
    "import pickle\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from random import sample\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "from sklearn import __version__ as skl_version\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a300006",
   "metadata": {},
   "source": [
    "## Initialize label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = numpy.load('classes.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d082eb",
   "metadata": {},
   "source": [
    "## Load RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"RNN.keras\"\n",
    "\n",
    "rnn = tf.keras.models.load_model(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842935d810303215",
   "metadata": {},
   "source": [
    "## Load CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f7276a8d077058",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"CNN.keras\"\n",
    "\n",
    "rnn = tf.keras.models.load_model(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e23ea2d912a579",
   "metadata": {},
   "source": [
    "## Load RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc4ce77b945c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "with open('rf_model.pkl', 'rb') as f:\n",
    "    random_forest = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb11dc0",
   "metadata": {},
   "source": [
    "## extract_features\n",
    "A function the extract the relevant features from the data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13307d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(xyz, sample_rate=64):\n",
    "    \"\"\"Extract commonly used HAR time-series features. xyz is a window of shape (N,3)\"\"\"\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    x, y, z = xyz.T\n",
    "\n",
    "    feats['xmin'], feats['xq25'], feats['xmed'], feats['xq75'], feats['xmax'] = np.quantile(x, (0, .25, .5, .75, 1))\n",
    "    feats['ymin'], feats['yq25'], feats['ymed'], feats['yq75'], feats['ymax'] = np.quantile(y, (0, .25, .5, .75, 1))\n",
    "    feats['zmin'], feats['zq25'], feats['zmed'], feats['zq75'], feats['zmax'] = np.quantile(z, (0, .25, .5, .75, 1))\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):  # ignore div by 0 warnings\n",
    "        # xy, xy, zx correlation\n",
    "        feats['xycorr'] = np.nan_to_num(np.corrcoef(x, y)[0, 1])\n",
    "        feats['yzcorr'] = np.nan_to_num(np.corrcoef(y, z)[0, 1])\n",
    "        feats['zxcorr'] = np.nan_to_num(np.corrcoef(z, x)[0, 1])\n",
    "\n",
    "    v = np.linalg.norm(xyz, axis=1)\n",
    "\n",
    "    feats['min'], feats['q25'], feats['med'], feats['q75'], feats['max'] = np.quantile(v, (0, .25, .5, .75, 1))\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):  # ignore div by 0 warnings\n",
    "        # 1s autocorrelation\n",
    "        feats['corr1s'] = np.nan_to_num(np.corrcoef(v[:-sample_rate], v[sample_rate:]))[0, 1]\n",
    "\n",
    "    # Angular features\n",
    "    feats.update(angular_features(xyz, sample_rate))\n",
    "\n",
    "    # Spectral features\n",
    "    feats.update(spectral_features(v, sample_rate))\n",
    "\n",
    "    # Peak features\n",
    "    feats.update(peak_features(v, sample_rate))\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def spectral_features(v, sample_rate):\n",
    "    \"\"\" Spectral entropy, 1st & 2nd dominant frequencies \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    # Spectrum using Welch's method with 3s segment length\n",
    "    # First run without detrending to get the true spectrum\n",
    "    freqs, powers = signal.welch(v, fs=sample_rate,\n",
    "                                 nperseg=3 * sample_rate,\n",
    "                                 noverlap=2 * sample_rate,\n",
    "                                 detrend=False,\n",
    "                                 average='median')\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):  # ignore div by 0 warnings\n",
    "        feats['pentropy'] = np.nan_to_num(stats.entropy(powers + 1e-16))\n",
    "\n",
    "    # Spectrum using Welch's method with 3s segment length\n",
    "    # Now do detrend to focus on the relevant freqs\n",
    "    freqs, powers = signal.welch(v, fs=sample_rate,\n",
    "                                 nperseg=3 * sample_rate,\n",
    "                                 noverlap=2 * sample_rate,\n",
    "                                 detrend='constant',\n",
    "                                 average='median')\n",
    "\n",
    "    peaks, _ = signal.find_peaks(powers)\n",
    "    peak_powers = powers[peaks]\n",
    "    peak_freqs = freqs[peaks]\n",
    "    peak_ranks = np.argsort(peak_powers)[::-1]\n",
    "    if len(peaks) >= 2:\n",
    "        feats['f1'] = peak_freqs[peak_ranks[0]]\n",
    "        feats['f2'] = peak_freqs[peak_ranks[1]]\n",
    "        feats['p1'] = peak_powers[peak_ranks[0]]\n",
    "        feats['p2'] = peak_powers[peak_ranks[1]]\n",
    "    elif len(peaks) == 1:\n",
    "        feats['f1'] = feats['f2'] = peak_freqs[peak_ranks[0]]\n",
    "        feats['p1'] = feats['p2'] = peak_powers[peak_ranks[0]]\n",
    "    else:\n",
    "        feats['f1'] = feats['f2'] = 0\n",
    "        feats['p1'] = feats['p2'] = 0\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def peak_features(v, sample_rate):\n",
    "    \"\"\" Features of the signal peaks. A proxy to step counts. \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "    u = butterfilt(v, (.6, 5), fs=sample_rate)\n",
    "    peaks, peak_props = signal.find_peaks(u, distance=0.2 * sample_rate, prominence=0.25)\n",
    "    feats['numPeaks'] = len(peaks)\n",
    "    if len(peak_props['prominences']) > 0:\n",
    "        feats['peakPromin'] = np.median(peak_props['prominences'])\n",
    "    else:\n",
    "        feats['peakPromin'] = 0\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def angular_features(xyz, sample_rate):\n",
    "    \"\"\" Roll, pitch, yaw.\n",
    "    Hip and Wrist Accelerometer Algorithms for Free-Living Behavior\n",
    "    Classification, Ellis et al.\n",
    "    \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    # Raw angles\n",
    "    x, y, z = xyz.T\n",
    "\n",
    "    roll = np.arctan2(y, z)\n",
    "    pitch = np.arctan2(x, z)\n",
    "    yaw = np.arctan2(y, x)\n",
    "\n",
    "    feats['avgroll'] = np.mean(roll)\n",
    "    feats['avgpitch'] = np.mean(pitch)\n",
    "    feats['avgyaw'] = np.mean(yaw)\n",
    "    feats['sdroll'] = np.std(roll)\n",
    "    feats['sdpitch'] = np.std(pitch)\n",
    "    feats['sdyaw'] = np.std(yaw)\n",
    "\n",
    "    # Gravity angles\n",
    "    xyz = butterfilt(xyz, 0.5, fs=sample_rate)\n",
    "\n",
    "    x, y, z = xyz.T\n",
    "\n",
    "    roll = np.arctan2(y, z)\n",
    "    pitch = np.arctan2(x, z)\n",
    "    yaw = np.arctan2(y, x)\n",
    "\n",
    "    feats['rollg'] = np.mean(roll)\n",
    "    feats['pitchg'] = np.mean(pitch)\n",
    "    feats['yawg'] = np.mean(yaw)\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def butterfilt(x, cutoffs, fs, order=10, axis=0):\n",
    "    nyq = 0.5 * fs\n",
    "    if isinstance(cutoffs, tuple):\n",
    "        hicut, lowcut = cutoffs\n",
    "        if hicut > 0:\n",
    "            btype = 'bandpass'\n",
    "            Wn = (hicut / nyq, lowcut / nyq)\n",
    "        else:\n",
    "            btype = 'low'\n",
    "            Wn = lowcut / nyq\n",
    "    else:\n",
    "        btype = 'low'\n",
    "        Wn = cutoffs / nyq\n",
    "    sos = signal.butter(order, Wn, btype=btype, analog=False, output='sos')\n",
    "    y = signal.sosfiltfilt(sos, x, axis=axis)\n",
    "    return y\n",
    "\n",
    "\n",
    "def get_feature_names():\n",
    "    \"\"\" Hacky way to get the list of feature names \"\"\"\n",
    "\n",
    "    feats = extract_features(np.zeros((1000, 3)), 100)\n",
    "    return list(feats.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a063703ff00692b",
   "metadata": {},
   "source": [
    "## extract_windows\n",
    "A function to create windows. We can also change the frequency of the windows by downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f2439e34c8f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_windows(data, winsize=90, input_frequency=64, output_frequency=64):\n",
    "    \"\"\"\n",
    "    Extracts sliding windows from time series data and labels for classification tasks.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Time series DataFrame with 'x', 'y', 'z' (accelerometer data) and 'label'.\n",
    "                             Index should be a datetime-like index.\n",
    "        winsize (int): Window size in seconds.\n",
    "        input_frequency (int): Sampling frequency of input data in Hz.\n",
    "        output_frequency (int): Desired output frequency in Hz (must be a divisor of input_frequency).\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Shape (n_samples, output_samples, 3), accelerometer windows.\n",
    "        Y (np.ndarray): Shape (n_samples,), most frequent label per window.\n",
    "    \"\"\"\n",
    "    # Calculate window size in samples and target output samples\n",
    "    window_samples = winsize * input_frequency\n",
    "    output_samples = winsize * output_frequency  # Expected downsampled length\n",
    "\n",
    "    X, Y = [], []\n",
    "\n",
    "    # Sliding window extraction\n",
    "    for start in range(0, len(data) - window_samples + 1, window_samples):\n",
    "        window = data.iloc[start:start + window_samples]\n",
    "\n",
    "        # Skip if missing values exist\n",
    "        if window.isna().any().any() or len(window) != window_samples:\n",
    "            continue\n",
    "\n",
    "        # Extract and resample accelerometer data\n",
    "        x = window[['x', 'y', 'z']].to_numpy()\n",
    "        x = resample(x, output_samples)  # Resample to match output frequency\n",
    "\n",
    "        # Extract the most frequent label (mode)\n",
    "        y = window['label'].mode().iloc[0]\n",
    "\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    X = np.stack(X) if X else np.empty((0, output_samples, 3))\n",
    "    Y = np.array(Y) if Y else np.empty((0,))\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c207753472fb8",
   "metadata": {},
   "source": [
    "## normalize\n",
    "Normalize the data to make it better for generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0844ea9ab1d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, feature_cols):\n",
    "    \"\"\"\n",
    "    Normalizes the specified feature columns of a DataFrame using Min-Max scaling.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): A DataFrame containing the features to be normalized.\n",
    "        feature_cols (list of str): List of column names in `data` to normalize.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the specified columns normalized to the range [0, 1].\n",
    "        The original `data` DataFrame is modified in place.\n",
    "        \n",
    "    Notes:\n",
    "        - Min-Max normalization scales each feature column to the range [0, 1] based on \n",
    "          the minimum and maximum values of the column.\n",
    "        - This transformation is often used to prepare data for machine learning models \n",
    "          that are sensitive to feature magnitudes.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    data[feature_cols] = scaler.fit_transform(data[feature_cols])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac39dff8b63abf9",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf9102379f1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_and_process_imu_data(directory, accel_freq=64):\n",
    "    # Define the directory containing the IMU data\n",
    "    imu_directory = os.path.join(directory, 'imu')\n",
    "    imu_path = Path(imu_directory)\n",
    "\n",
    "    # Flags to track processed hands\n",
    "    l_hand = False\n",
    "    r_hand = False\n",
    "    \n",
    "    # DataFrames to hold IMU data for left and right wrists\n",
    "    imu_data_l = None\n",
    "    imu_data_r = None\n",
    "    \n",
    "    # Iterate over all XML files in the directory\n",
    "    for file_path in imu_path.rglob(\"*.xml\"):\n",
    "        # Get the directory containing the unisens.xml\n",
    "        unisens_directory = os.path.dirname(file_path)\n",
    "        u = unisens.Unisens(unisens_directory)\n",
    "\n",
    "        # Extract raw accelerometer and gyroscope data\n",
    "        raw_accel_data = u['acc.bin'].get_data()\n",
    "        metadata = u['customAttributes']\n",
    "        \n",
    "        # Extract metadata attributes\n",
    "        metadata_dict = vars(metadata)\n",
    "        sensorLocation = metadata_dict['attrib']['sensorLocation']\n",
    "\n",
    "        # Skip processing if data for the same wrist has already been processed\n",
    "        if sensorLocation == 'left_wrist' and l_hand:\n",
    "            print(f\"Skipping already processed left wrist data in {file_path}\")\n",
    "            continue\n",
    "        if sensorLocation == 'right_wrist' and r_hand:\n",
    "            print(f\"Skipping already processed right wrist data in {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Parse imu_timestampStartUTC\n",
    "        imu_timestampStartUTC = pd.to_datetime(metadata_dict['attrib']['timestampStartUTC'], format='%Y-%m-%dT%H:%M:%S.%f')\n",
    "        timezone_offset_seconds = int(metadata_dict['attrib']['timeZoneOffset'])\n",
    "        imu_timestampStartUTC += pd.Timedelta(seconds=timezone_offset_seconds)\n",
    "\n",
    "        # Generate timestamps efficiently using pd.date_range\n",
    "        num_samples = len(raw_accel_data[0])\n",
    "        timestamps = pd.date_range(start=imu_timestampStartUTC, periods=num_samples, freq=f\"{int(1 / accel_freq * 1e3)}ms\")\n",
    "        \n",
    "        # Prepare IMU data for the left wrist\n",
    "        if sensorLocation == 'left_wrist':\n",
    "            l_hand = True\n",
    "            imu_data_l = pd.DataFrame({\n",
    "                'timestamp': timestamps,\n",
    "                'x': raw_accel_data[0],  # accelerometer x\n",
    "                'y': raw_accel_data[1],  # accelerometer y\n",
    "                'z': raw_accel_data[2],  # accelerometer z\n",
    "            })\n",
    "        \n",
    "        # Prepare IMU data for the right wrist\n",
    "        if sensorLocation == 'right_wrist':\n",
    "            r_hand = True\n",
    "            imu_data_r = pd.DataFrame({\n",
    "                'timestamp': timestamps,\n",
    "                'x': raw_accel_data[0],  # accelerometer x\n",
    "                'y': raw_accel_data[1],  # accelerometer y\n",
    "                'z': raw_accel_data[2],  # accelerometer z\n",
    "            })\n",
    "    \n",
    "        if sensorLocation == 'left_wrist':\n",
    "            imu_data_l['timestamp'] = pd.to_datetime(imu_data_l['timestamp'])\n",
    "        \n",
    "        if sensorLocation == 'right_wrist':\n",
    "            imu_data_r['timestamp'] = pd.to_datetime(imu_data_r['timestamp'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Return the processed IMU data\n",
    "    return imu_data_l, imu_data_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a093d9a466fa8",
   "metadata": {},
   "source": [
    "## Apply the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faccdf2aa1dadaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMU data\n",
    "imu_data_l, imu_data_r = import_and_process_imu_data(\"/Volumes/T7/eXprt-backup/05022024/data/MP-P002/\")\n",
    "imu_data_l.set_index('timestamp', inplace=True)\n",
    "imu_data_r.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Normalize the accelerometer data\n",
    "imu_data_l = normalize(imu_data_l, ['x', 'y', 'z'])\n",
    "\n",
    "# Extract windows from the accelerometer data\n",
    "X_l = extract_windows(imu_data_l, winsize=90, input_frequency=64, output_frequency=64)\n",
    "\n",
    "# Apply the model to the extracted windows\n",
    "predictions_l_prob = rnn.predict(X_l)\n",
    "predictions_l = label_encoder.inverse_transform(predictions_l_prob.argmax(axis=1))\n",
    "\n",
    "# Normalize the accelerometer data\n",
    "imu_data_r = normalize(imu_data_r, ['x', 'y', 'z'])\n",
    "\n",
    "# Extract windows from the accelerometer data\n",
    "X_r = extract_windows(imu_data_r, winsize=90, input_frequency=64, output_frequency=64)\n",
    "\n",
    "# Apply the model to the extracted windows\n",
    "predictions_r_prob = rnn.predict(X_r)\n",
    "predictions_r = label_encoder.inverse_transform(predictions_r_prob.argmax(axis=1))\n",
    "\n",
    "# Compare the two wrists\n",
    "print(predictions_l)\n",
    "print(predictions_r)\n",
    "\n",
    "min_length = min(len(predictions_l), len(predictions_r))\n",
    "predictions_l = predictions_l[:min_length]\n",
    "predictions_r = predictions_r[:min_length]\n",
    "\n",
    "# Calculate the percentage of time the model predicts the same activity for both wrists\n",
    "same_activity = np.mean(predictions_l == predictions_r)\n",
    "\n",
    "print(f\"Percentage of time the model predicts the same activity for both wrists: {same_activity:.2%}\")\n",
    "\n",
    "predictions_l_prob = predictions_l_prob[:min_length]\n",
    "predictions_r_prob = predictions_r_prob[:min_length]\n",
    "\n",
    "# Calculate probality similarity\n",
    "l2 = np.linalg.norm(predictions_l_prob - predictions_r_prob)\n",
    "\n",
    "print(f\"Probability similarity between the two wrists: {l2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863954a3",
   "metadata": {},
   "source": [
    "## Apply the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5be9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMU data\n",
    "imu_data_l, imu_data_r = import_and_process_imu_data(\"/Volumes/T7/eXprt-backup/05022024/data/MP-P002/\")\n",
    "imu_data_l.set_index('timestamp', inplace=True)\n",
    "imu_data_r.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Normalize the accelerometer data\n",
    "imu_data_l = normalize(imu_data_l, ['x', 'y', 'z'])\n",
    "\n",
    "# Extract windows from the accelerometer data\n",
    "X_l = extract_windows(imu_data_l, winsize=90, input_frequency=64, output_frequency=64)\n",
    "\n",
    "# Apply the model to the extracted windows\n",
    "predictions_l_prob = cnn.predict(X_l)\n",
    "predictions_l = label_encoder.inverse_transform(predictions_l_prob.argmax(axis=1))\n",
    "\n",
    "# Normalize the accelerometer data\n",
    "imu_data_r = normalize(imu_data_r, ['x', 'y', 'z'])\n",
    "\n",
    "# Extract windows from the accelerometer data\n",
    "X_r = extract_windows(imu_data_r, winsize=90, input_frequency=64, output_frequency=64)\n",
    "\n",
    "# Apply the model to the extracted windows\n",
    "predictions_r_prob = cnn.predict(X_r)\n",
    "predictions_r = label_encoder.inverse_transform(predictions_r_prob.argmax(axis=1))\n",
    "\n",
    "# Compare the two wrists\n",
    "print(predictions_l)\n",
    "print(predictions_r)\n",
    "\n",
    "min_length = min(len(predictions_l), len(predictions_r))\n",
    "predictions_l = predictions_l[:min_length]\n",
    "predictions_r = predictions_r[:min_length]\n",
    "\n",
    "# Calculate the percentage of time the model predicts the same activity for both wrists\n",
    "same_activity = np.mean(predictions_l == predictions_r)\n",
    "\n",
    "print(f\"Percentage of time the model predicts the same activity for both wrists: {same_activity:.2%}\")\n",
    "\n",
    "predictions_l_prob = predictions_l_prob[:min_length]\n",
    "predictions_r_prob = predictions_r_prob[:min_length]\n",
    "\n",
    "# Calculate probality similarity\n",
    "l2 = np.linalg.norm(predictions_l_prob - predictions_r_prob)\n",
    "\n",
    "print(f\"Probability similarity between the two wrists: {l2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51323ccb",
   "metadata": {},
   "source": [
    "## Apply the Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMU data\n",
    "imu_data_l, imu_data_r = import_and_process_imu_data(\"/Volumes/T7/eXprt-backup/05022024/data/MP-P002/\")\n",
    "imu_data_l.set_index('timestamp', inplace=True)\n",
    "imu_data_r.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Normalize the accelerometer data\n",
    "imu_data_l = normalize(imu_data_l, ['x', 'y', 'z'])\n",
    "\n",
    "# Extract windows from the accelerometer data\n",
    "X_l = extract_windows(imu_data_l, winsize=90, input_frequency=64)\n",
    "X_l_feats = pd.DataFrame([extract_features(x) for x in X_l])\n",
    "\n",
    "# Apply the model to the extracted windows\n",
    "predictions_l_prob = random_forest.predict_proba(X_l_feats)\n",
    "\n",
    "# Normalize the accelerometer data\n",
    "imu_data_r = normalize(imu_data_r, ['x', 'y', 'z'])\n",
    "\n",
    "# Extract windows from the accelerometer data\n",
    "X_r = extract_windows(imu_data_r, winsize=90, input_frequency=64)\n",
    "X_r_feats = pd.DataFrame([extract_features(x) for x in X_r])\n",
    "\n",
    "# Apply the model to the extracted windows\n",
    "predictions_r_prob = random_forest.predict_proba(X_r_feats)\n",
    "\n",
    "# Ensure both arrays have the same length\n",
    "min_length = min(len(predictions_l_prob), len(predictions_r_prob))\n",
    "predictions_l_prob = predictions_l_prob[:min_length]\n",
    "predictions_r_prob = predictions_r_prob[:min_length]\n",
    "\n",
    "# Compare the two wrists\n",
    "print(predictions_l_prob)\n",
    "print(predictions_r_prob)\n",
    "\n",
    "predictions_l = random_forest.predict(X_l_feats)[:min_length]\n",
    "predictions_r = random_forest.predict(X_r_feats)[:min_length]\n",
    "\n",
    "# Compare predictions\n",
    "print(predictions_l)\n",
    "print(predictions_r)\n",
    "\n",
    "# Calculate the percentage of time the model predicts the same activity for both wrists\n",
    "same_activity = np.mean(predictions_l == predictions_r)\n",
    "\n",
    "print(f\"Percentage of time the model predicts the same activity for both wrists: {same_activity:.2%}\")\n",
    "\n",
    "# Calculate probability similarity\n",
    "l2 = np.linalg.norm(predictions_l_prob - predictions_r_prob)\n",
    "\n",
    "print(f\"Probability similarity between the two wrists: {l2:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
