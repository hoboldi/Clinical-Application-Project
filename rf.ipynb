{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 8639177,
     "sourceType": "datasetVersion",
     "datasetId": 5173716
    }
   ],
   "dockerImageVersionId": 30886,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Imports",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from random import sample\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from scipy.signal import resample\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as signal"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions\n",
    "## map_labels\n",
    "This function can be used to simplify the labels. With a given label dictionary, we can map the labels to a less specific label."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def map_labels(data, label_dict, label_mapping):\n    \"\"\"\n    Maps annotations in a dataset to their corresponding labels using a given mapping.\n\n    Args:\n        data (pd.DataFrame): A DataFrame containing an 'annotation' column with values to be mapped.\n        label_dict (dict): A dictionary containing mappings of annotation labels.\n                                Keys are label mapping names, and values are Pandas Series or DataFrames\n                                where the index represents annotation keys and values represent labels.\n        label_mapping (str): The key in `anno_label_dict` corresponding to the desired label mapping.\n\n    Returns:\n        pd.DataFrame: The input DataFrame with an added or updated 'label' column containing the mapped labels.\n    \"\"\"\n    data['label'] = (label_dict[label_mapping].reindex(data['annotation']).to_numpy())\n    return data",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## normalize\n",
    "Normalize the data to make it better for generalization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def normalize(data, feature_cols):\n    \"\"\"\n    Normalizes the specified feature columns of a DataFrame using Min-Max scaling.\n\n    Args:\n        data (pd.DataFrame): A DataFrame containing the features to be normalized.\n        feature_cols (list of str): List of column names in `data` to normalize.\n        \n    Returns:\n        pd.DataFrame: A new DataFrame with the specified columns normalized to the range [0, 1].\n        The original `data` DataFrame is modified in place.\n        \n    Notes:\n        - Min-Max normalization scales each feature column to the range [0, 1] based on \n          the minimum and maximum values of the column.\n        - This transformation is often used to prepare data for machine learning models \n          that are sensitive to feature magnitudes.\n    \"\"\"\n    scaler = MinMaxScaler()\n    data[feature_cols] = scaler.fit_transform(data[feature_cols])\n    return data",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## extract_windows\n",
    "A function to create windows. We can also change the frequency of the windows by downsampling."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def extract_windows(data, winsize=90, input_frequency=100, output_frequency=64):\n    \"\"\"\n    Extracts sliding windows from time series data and labels for classification tasks.\n\n    Args:\n        data (pd.DataFrame): Time series DataFrame with 'x', 'y', 'z' (accelerometer data) and 'label'.\n                             Index should be a datetime-like index.\n        winsize (int): Window size in seconds.\n        input_frequency (int): Sampling frequency of input data in Hz.\n        output_frequency (int): Desired output frequency in Hz (must be a divisor of input_frequency).\n\n    Returns:\n        X (np.ndarray): Shape (n_samples, output_samples, 3), accelerometer windows.\n        Y (np.ndarray): Shape (n_samples,), most frequent label per window.\n    \"\"\"\n    # Calculate window size in samples and target output samples\n    window_samples = winsize * input_frequency\n    output_samples = winsize * output_frequency  # Expected downsampled length\n\n    X, Y = [], []\n\n    # Sliding window extraction\n    for start in range(0, len(data) - window_samples + 1, window_samples):\n        window = data.iloc[start:start + window_samples]\n\n        # Skip if missing values exist\n        if window.isna().any().any() or len(window) != window_samples:\n            continue\n\n        # Extract and resample accelerometer data\n        x = window[['x', 'y', 'z']].to_numpy()\n        x = resample(x, output_samples)  # Resample to match output frequency\n\n        # Extract the most frequent label (mode)\n        y = window['label'].mode().iloc[0]\n\n        X.append(x)\n        Y.append(y)\n\n    X = np.stack(X) if X else np.empty((0, output_samples, 3))\n    Y = np.array(Y) if Y else np.empty((0,))\n\n    return X, Y",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## extract_features\n",
    "A function the extract the relevant features from the data for the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def extract_features(xyz, sample_rate=64):\n    \"\"\"Extract commonly used HAR time-series features. xyz is a window of shape (N,3)\"\"\"\n\n    feats = {}\n\n    x, y, z = xyz.T\n\n    feats['xmin'], feats['xq25'], feats['xmed'], feats['xq75'], feats['xmax'] = np.quantile(x, (0, .25, .5, .75, 1))\n    feats['ymin'], feats['yq25'], feats['ymed'], feats['yq75'], feats['ymax'] = np.quantile(y, (0, .25, .5, .75, 1))\n    feats['zmin'], feats['zq25'], feats['zmed'], feats['zq75'], feats['zmax'] = np.quantile(z, (0, .25, .5, .75, 1))\n\n    with np.errstate(divide='ignore', invalid='ignore'):  # ignore div by 0 warnings\n        # xy, xy, zx correlation\n        feats['xycorr'] = np.nan_to_num(np.corrcoef(x, y)[0, 1])\n        feats['yzcorr'] = np.nan_to_num(np.corrcoef(y, z)[0, 1])\n        feats['zxcorr'] = np.nan_to_num(np.corrcoef(z, x)[0, 1])\n\n    v = np.linalg.norm(xyz, axis=1)\n\n    feats['min'], feats['q25'], feats['med'], feats['q75'], feats['max'] = np.quantile(v, (0, .25, .5, .75, 1))\n\n    with np.errstate(divide='ignore', invalid='ignore'):  # ignore div by 0 warnings\n        # 1s autocorrelation\n        feats['corr1s'] = np.nan_to_num(np.corrcoef(v[:-sample_rate], v[sample_rate:]))[0, 1]\n\n    # Angular features\n    feats.update(angular_features(xyz, sample_rate))\n\n    # Spectral features\n    feats.update(spectral_features(v, sample_rate))\n\n    # Peak features\n    feats.update(peak_features(v, sample_rate))\n\n    return feats\n\n\ndef spectral_features(v, sample_rate):\n    \"\"\" Spectral entropy, 1st & 2nd dominant frequencies \"\"\"\n\n    feats = {}\n\n    # Spectrum using Welch's method with 3s segment length\n    # First run without detrending to get the true spectrum\n    freqs, powers = signal.welch(v, fs=sample_rate,\n                                 nperseg=3 * sample_rate,\n                                 noverlap=2 * sample_rate,\n                                 detrend=False,\n                                 average='median')\n\n    with np.errstate(divide='ignore', invalid='ignore'):  # ignore div by 0 warnings\n        feats['pentropy'] = np.nan_to_num(stats.entropy(powers + 1e-16))\n\n    # Spectrum using Welch's method with 3s segment length\n    # Now do detrend to focus on the relevant freqs\n    freqs, powers = signal.welch(v, fs=sample_rate,\n                                 nperseg=3 * sample_rate,\n                                 noverlap=2 * sample_rate,\n                                 detrend='constant',\n                                 average='median')\n\n    peaks, _ = signal.find_peaks(powers)\n    peak_powers = powers[peaks]\n    peak_freqs = freqs[peaks]\n    peak_ranks = np.argsort(peak_powers)[::-1]\n    if len(peaks) >= 2:\n        feats['f1'] = peak_freqs[peak_ranks[0]]\n        feats['f2'] = peak_freqs[peak_ranks[1]]\n        feats['p1'] = peak_powers[peak_ranks[0]]\n        feats['p2'] = peak_powers[peak_ranks[1]]\n    elif len(peaks) == 1:\n        feats['f1'] = feats['f2'] = peak_freqs[peak_ranks[0]]\n        feats['p1'] = feats['p2'] = peak_powers[peak_ranks[0]]\n    else:\n        feats['f1'] = feats['f2'] = 0\n        feats['p1'] = feats['p2'] = 0\n\n    return feats\n\n\ndef peak_features(v, sample_rate):\n    \"\"\" Features of the signal peaks. A proxy to step counts. \"\"\"\n\n    feats = {}\n    u = butterfilt(v, (.6, 5), fs=sample_rate)\n    peaks, peak_props = signal.find_peaks(u, distance=0.2 * sample_rate, prominence=0.25)\n    feats['numPeaks'] = len(peaks)\n    if len(peak_props['prominences']) > 0:\n        feats['peakPromin'] = np.median(peak_props['prominences'])\n    else:\n        feats['peakPromin'] = 0\n\n    return feats\n\n\ndef angular_features(xyz, sample_rate):\n    \"\"\" Roll, pitch, yaw.\n    Hip and Wrist Accelerometer Algorithms for Free-Living Behavior\n    Classification, Ellis et al.\n    \"\"\"\n\n    feats = {}\n\n    # Raw angles\n    x, y, z = xyz.T\n\n    roll = np.arctan2(y, z)\n    pitch = np.arctan2(x, z)\n    yaw = np.arctan2(y, x)\n\n    feats['avgroll'] = np.mean(roll)\n    feats['avgpitch'] = np.mean(pitch)\n    feats['avgyaw'] = np.mean(yaw)\n    feats['sdroll'] = np.std(roll)\n    feats['sdpitch'] = np.std(pitch)\n    feats['sdyaw'] = np.std(yaw)\n\n    # Gravity angles\n    xyz = butterfilt(xyz, 0.5, fs=sample_rate)\n\n    x, y, z = xyz.T\n\n    roll = np.arctan2(y, z)\n    pitch = np.arctan2(x, z)\n    yaw = np.arctan2(y, x)\n\n    feats['rollg'] = np.mean(roll)\n    feats['pitchg'] = np.mean(pitch)\n    feats['yawg'] = np.mean(yaw)\n\n    return feats\n\n\ndef butterfilt(x, cutoffs, fs, order=10, axis=0):\n    nyq = 0.5 * fs\n    if isinstance(cutoffs, tuple):\n        hicut, lowcut = cutoffs\n        if hicut > 0:\n            btype = 'bandpass'\n            Wn = (hicut / nyq, lowcut / nyq)\n        else:\n            btype = 'low'\n            Wn = lowcut / nyq\n    else:\n        btype = 'low'\n        Wn = cutoffs / nyq\n    sos = signal.butter(order, Wn, btype=btype, analog=False, output='sos')\n    y = signal.sosfiltfilt(sos, x, axis=axis)\n    return y\n\n\ndef get_feature_names():\n    \"\"\" Hacky way to get the list of feature names \"\"\"\n\n    feats = extract_features(np.zeros((1000, 3)), 100)\n    return list(feats.keys())",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Print input files",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Dictionary of labels\nlabel_dict = pd.read_csv('/kaggle/input/capture-24-human-activity-recognition/capture24/annotation-label-dictionary.csv', index_col='annotation', dtype='string')\nprint(\"Annotation-Label Dictionary\")\nprint(label_dict)\n\n# Chosen label mapping\nlabel_mapping = \"label:Willetts2018\"\nprint(label_dict[label_mapping])\nprint()\n\n# Print files\nprint('Content of data/')\nprint(sorted(os.listdir('/kaggle/input/capture-24-human-activity-recognition/capture24')))\nprint()\n\n# All of the labels\nall_labels = list({*label_dict[label_mapping]})\nprint('All of the possible labels:')\nprint(all_labels)",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Training the model\n## Training parameters",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Path of the dataset\npath = '/kaggle/input/capture-24-human-activity-recognition/capture24/'\n\n# The files in the dataset\nfile_list = sorted([f for f in os.listdir(path) if f.endswith('.csv') and f.startswith('P')])\n\n# Stopping criterions for testing\ncounter = 0\nlimit = 200\nlimit = min(limit, len(file_list))\nprint(\"Number of files used: \", limit)\n\n# Number of files used purely for testing\nnum_test_files = 20\nprint(\"Number of test files: \", num_test_files)\n\n# Randomly select test files from the file list\ntest_files = sample(file_list, num_test_files)\n\n# Remaning files used for training\ntrain_files = [f for f in file_list if f not in test_files]",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Loading the data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Load and process data in chunks to manage memory\n",
    "data_list = []\n",
    "counter = 0\n",
    "\n",
    "X, Y = [], []\n",
    "\n",
    "# Iterate over the training files\n",
    "for file in train_files:\n",
    "    counter += 1\n",
    "    print(\"Processing File:\", counter)\n",
    "    \n",
    "    file_path = os.path.join(path, file)\n",
    "    file_path = os.path.join(file_path, file)\n",
    "    data = pd.read_csv(file_path, index_col='time', parse_dates=['time'],\n",
    "                           dtype={'x': 'f4', 'y': 'f4', 'z': 'f4', 'annotation': 'string'})\n",
    "    \n",
    "    data = map_labels(data, label_dict, label_mapping)      \n",
    "    data = normalize(data, ['x','y','z'])\n",
    "    \n",
    "    X_, Y_ = extract_windows(data)\n",
    "    \n",
    "    # Free memory\n",
    "    del data\n",
    "    \n",
    "    X_feats = pd.DataFrame([extract_features(x) for x in X_])\n",
    "    \n",
    "    # Free memory\n",
    "    del X_ \n",
    "    \n",
    "    X.append(X_feats)\n",
    "    Y.append(Y_)\n",
    "    \n",
    "    # Free memory\n",
    "    del X_feats, Y_ \n",
    "\n",
    "X_feats_train = pd.concat(X, ignore_index=True)\n",
    "Y_train = np.concatenate(Y, axis=0)\n",
    "\n",
    "# Free memory\n",
    "del X, Y\n",
    "\n",
    "print('Data preprocessing done.\\n')"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Training the model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "\n",
    "# Create the model\n",
    "model = BalancedRandomForestClassifier(\n",
    "    random_state=0, \n",
    "    n_estimators=3000, \n",
    "    criterion='gini'\n",
    ")\n",
    "\n",
    "# Use Stratified K-Fold Cross-Validation\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    estimator=model,\n",
    "    X=X_feats_train,\n",
    "    y=Y_train,\n",
    "    scoring='accuracy',\n",
    "    cv=cv,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(f'Cross-validation scores: {cv_scores}')\n",
    "print(f'Mean cross-validation score: {cv_scores.mean()}')\n",
    "print(f'Standard deviation of cross-validation score: {cv_scores.std()}')\n",
    "\n",
    "model.classes_ = np.array(all_labels)\n",
    "model.fit(X_feats_train, Y_train)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-26T15:10:27.414461Z",
     "iopub.execute_input": "2025-02-26T15:10:27.414715Z",
     "iopub.status.idle": "2025-02-26T15:10:27.500306Z",
     "shell.execute_reply.started": "2025-02-26T15:10:27.414689Z",
     "shell.execute_reply": "2025-02-26T15:10:27.498921Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Testing the model",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Loading the data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Load and concatenate data from all files\n",
    "\n",
    "data_list = []\n",
    "\n",
    "# Iterate over the test files\n",
    "for file in test_files:\n",
    "    \n",
    "    counter += 1\n",
    "    file_path = os.path.join(path, file)\n",
    "    file_path = os.path.join(file_path, file)\n",
    "    data = pd.read_csv(file_path, index_col='time', parse_dates=['time'],\n",
    "                           dtype={'x': 'f4', 'y': 'f4', 'z': 'f4', 'annotation': 'string'})\n",
    "\n",
    "    data = map_labels(data, label_dict, label_mapping)      \n",
    "    data = normalize(data, ['x','y','z'])\n",
    "    data_list.append(data)\n",
    "\n",
    "    # Map the label\n",
    "    map_labels(data, label_dict, label_mapping)\n",
    "    data_list.append(data)\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for data in data_list:\n",
    "    X_, Y_ = extract_windows(data)\n",
    "    X.append(X_)\n",
    "    Y.append(Y_)\n",
    "\n",
    "X_feats = []\n",
    "\n",
    "for X_ in X:\n",
    "    X_feats.append(pd.DataFrame([\n",
    "        extract_features(x) for x in X_]))\n",
    "\n",
    "X_feats_test = pd.concat(X_feats, ignore_index=True)\n",
    "Y_test = np.concatenate(Y, axis=0)\n",
    "\n",
    "print('Data preprocessing done.\\n')"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Testing the model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "Y_pred = model.predict(X_feats_test)\n\n# Calculate the important metrics\naccuracy = accuracy_score(Y_test, Y_pred)\nf1_score = f1_score(Y_test, Y_pred, average='weighted')\nprecision = precision_score(Y_test, Y_pred, average='weighted')\nrecall = recall_score(Y_test, Y_pred, average='weighted')\n\nprint(f'Accuracy: {accuracy}')\nprint(f'F1 score: {model.score(X_feats_test, Y_test)}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\nprint(f'Confusion matrix: {model.classes_}')\n\n# Subset labels present in the test set\npresent_labels = np.unique(Y_test)\n\n# Confusion matrix for present labels\ncm = confusion_matrix(Y_test, Y_pred, labels=present_labels, normalize='true')\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=present_labels)\ndisp.plot(cmap='Blues', values_format=\".2%\")\ndisp.ax_.set_title(\"Confusion Matrix (Percentage)\")\n\nplt.show()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Save the model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Save the model\nwith open('rf_model.pkl', 'wb') as f:\n    pickle.dump(model, f)",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
